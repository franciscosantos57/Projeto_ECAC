================================================================================
PROJETO ECAC - ANÁLISES E CONCLUSÕES
Engenharia de Características para Classificação de Atividades Humanas
================================================================================

Este documento contém todas as análises, interpretações e conclusões dos
exercícios realizados no projeto. As explicações foram removidas do terminal
para tornar a execução mais limpa e focada no progresso.

================================================================================
EXERCÍCIO 3.2: ANÁLISE DE DENSIDADE DE OUTLIERS (MÉTODO IQR)
================================================================================

OBJETIVO:
Analisar a densidade de outliers usando o método IQR de Tukey, focando apenas
nos sensores do pulso direito.

PADRÕES IDENTIFICADOS:

1. ATIVIDADES COM MAIOR DENSIDADE DE OUTLIERS:
   - Transições entre atividades tendem a ter mais outliers
   - Atividades com movimentos bruscos ou irregulares
   - Exemplos: "Stand->Sit", "Sit->Stand", "Climb Stair"

2. ATIVIDADES COM MENOR DENSIDADE DE OUTLIERS:
   - Atividades estáticas ou com movimentos regulares
   - Exemplos: "Stand", "Sit", "Walk"

3. DISTRIBUIÇÃO POR SENSOR:
   - Acelerómetro: Capta bem movimentos de translação
   - Giroscópio: Sensível a rotações e mudanças de orientação
   - Magnetómetro: Menos outliers, valores mais estáveis

INTERPRETAÇÃO:
O método IQR é eficaz para identificar valores anómalos em atividades.
As transições entre estados são naturalmente mais irregulares, gerando
mais outliers. Isto é esperado e não indica necessariamente erro nos dados.

================================================================================
EXERCÍCIO 3.5: COMPARAÇÃO IQR vs Z-SCORE
================================================================================

ANÁLISE COMPARATIVA DOS MÉTODOS:

1. MÉTODO IQR (INTERQUARTILE RANGE):
   Vantagens:
   - Robusto a outliers extremos
   - Não assume distribuição normal
   - Baseado em percentis (Q1, Q3)
   - Bom para dados assimétricos
   
   Desvantagens:
   - Pode ser conservador (detecta menos outliers)
   - Sensível a distribuições com caudas longas

2. MÉTODO Z-SCORE:
   Vantagens:
   - Sensível a desvios da média
   - Paramétrico (assume normalidade)
   - Ajustável via parâmetro k (tipicamente 3, 3.5 ou 4)
   
   Desvantagens:
   - Assume distribuição normal
   - Menos robusto a outliers extremos
   - Pode detectar muitos ou poucos outliers conforme k

RESULTADOS OBSERVADOS:

- Z-Score (k=3): Detecta MAIS outliers que IQR
  → Mais sensível, menos conservador
  
- Z-Score (k=4): Detecta MENOS outliers que IQR
  → Mais conservador, menos sensível
  
- IQR: Comportamento intermédio
  → Equilíbrio entre sensibilidade e robustez

RECOMENDAÇÕES:

- Para detecção de anomalias graves: usar k=4 (mais conservador)
- Para análise exploratória: usar IQR ou k=3
- Para dados com distribuição normal: Z-Score é mais eficiente
- Para dados não-normais: IQR é mais adequado
- Para análise robusta: usar ambos os métodos e comparar

CONCLUSÃO:
A escolha do método depende do contexto e objectivos da análise. O IQR é
geralmente mais robusto e não assume normalidade, enquanto o Z-Score oferece
maior controlo através do parâmetro k.

================================================================================
EXERCÍCIO 3.6 e 3.7: K-MEANS PARA DETECÇÃO DE OUTLIERS
================================================================================

METODOLOGIA:

1. Normalização dos dados usando Z-Score
2. Aplicação do algoritmo K-Means com k=3, 5, 7 clusters
3. Detecção de outliers usando método IQR nas distâncias aos centroides
4. Visualização 3D no espaço dos módulos dos sensores

RESULTADOS POR NÚMERO DE CLUSTERS:

k=3: Agrupamento mais grosseiro
- Outliers representam extremos mais pronunciados
- Boa separação visual dos clusters
- Adequado para classificação de alto nível

k=5: Equilíbrio entre detalhe e generalização
- Captura mais nuances nas atividades
- Outliers são desvios moderados
- Recomendado para análise inicial

k=7: Agrupamento mais fino
- Maior granularidade na detecção
- Mais sensível a variações subtis
- Pode sobre-segmentar os dados

INTERPRETAÇÃO DAS DISTÂNCIAS:

- Distâncias pequenas: Pontos típicos da atividade
- Distâncias moderadas: Variações normais
- Distâncias grandes (outliers): Comportamentos atípicos ou transições

COMPARAÇÃO COM Z-SCORE:

O K-Means oferece uma perspectiva diferente:
- Z-Score: Baseado em desvios univariados da média
- K-Means: Baseado em distâncias multivariadas aos centroides

Ambos são complementares e capturam tipos diferentes de anomalias.

APLICAÇÕES:

- Pré-processamento: Remover outliers antes de treinar modelos
- Análise exploratória: Identificar padrões e sub-grupos
- Detecção de anomalias: Sinalizar comportamentos anómalos

================================================================================
EXERCÍCIO 3.7.1: DBSCAN PARA DETECÇÃO DE OUTLIERS
================================================================================

METODOLOGIA:

DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
- Algoritmo baseado em densidade
- Identifica clusters de forma arbitrária
- Marca pontos isolados como noise/outliers

PARÂMETROS TESTADOS:

1. eps=0.5, min_samples=5
   - Raio de vizinhança pequeno
   - Mais restritivo
   - Detecta mais outliers

2. eps=0.8, min_samples=5
   - Raio de vizinhança maior
   - Menos restritivo
   - Detecta menos outliers

RESULTADOS:

- DBSCAN é sensível aos parâmetros eps e min_samples
- Outliers são pontos que não pertencem a nenhum cluster denso
- Adequado para dados com clusters de densidade variável

COMPARAÇÃO DBSCAN vs K-MEANS:

K-Means:
- Assume clusters esféricos
- Número de clusters definido à priori
- Todos os pontos pertencem a algum cluster

DBSCAN:
- Detecta clusters de forma arbitrária
- Número de clusters determinado automaticamente
- Identifica explicitamente outliers (noise)

RECOMENDAÇÕES:

- DBSCAN: Para dados com clusters de densidade variável
- K-Means: Para dados com clusters bem definidos e esféricos
- Usar ambos para análise mais completa

================================================================================
EXERCÍCIO 4.1: SIGNIFICÂNCIA ESTATÍSTICA
================================================================================

OBJETIVO:
Determinar se os valores médios dos módulos dos sensores diferem
significativamente entre as diferentes atividades.

METODOLOGIA:

1. TESTE DE NORMALIDADE (KOLMOGOROV-SMIRNOV)
   - Hipótese nula (H0): Os dados seguem distribuição normal
   - Critério: p > 0.05 indica normalidade
   - Resultado: A maioria das distribuições NÃO é normal

2. ESCOLHA DO TESTE ESTATÍSTICO
   - Se todas forem normais: ANOVA (paramétrico)
   - Se alguma não for normal: Kruskal-Wallis (não-paramétrico)
   
3. TESTE DE SIGNIFICÂNCIA
   - Hipótese nula (H0): Médias iguais entre atividades
   - Critério: p < 0.05 indica diferenças significativas

RESULTADOS:

Para os 3 sensores (Acelerómetro, Giroscópio, Magnetómetro):
- p-values < 0.001 (altamente significativos)
- Conclusão: As médias diferem SIGNIFICATIVAMENTE entre atividades

INTERPRETAÇÃO:

BOM DISCRIMINANTE:
- Os módulos dos sensores têm poder discriminante
- Podem ser usados como features para classificação
- As diferenças entre atividades são estatisticamente robustas

IMPLICAÇÕES PARA MACHINE LEARNING:

- Features baseadas em módulos são informativas
- Justifica o uso destes sensores para classificação
- Diferenças significativas facilitam a separabilidade das classes

COMENTÁRIOS TÉCNICOS:

- Muitas distribuições não são normais (justifica testes não-paramétricos)
- Kruskal-Wallis é mais robusto que ANOVA para estes dados
- p-values extremamente baixos indicam diferenças muito claras
- Resultados consistentes com intuição física das atividades

================================================================================
EXERCÍCIO 4.2: EXTRAÇÃO DE FEATURES
================================================================================

OBJETIVO:
Extrair features temporais e espectrais para comprimir o espaço do problema
e criar um dataset pronto para algoritmos de classificação.

METODOLOGIA:

1. SLIDING WINDOWS:
   - Janelas de 5 segundos
   - Overlap de 50%
   - 250 amostras por janela (50 Hz × 5s)
   - Descarta janelas com múltiplas atividades

2. FEATURES TEMPORAIS (14 por sensor - Tabela 1):
   - mean: Valor médio do sinal (DC component)
   - median: Mediana do sinal
   - std: Desvio padrão (Standard Deviation)
   - variance: Variância do sinal
   - min/max: Valores extremos
   - range: Amplitude (max - min)
   - rms: Root Mean Square (quadratic mean value)
   - mad: Mean Absolute Deviation (averaged derivatives)
   - iqr: Interquartile Range (75th - 25th percentis)
   - skewness: Assimetria da distribuição
   - kurtosis: Curtose (achatamento/peakedness)
   - zcr: Zero Crossing Rate (mudanças de sinal)
   - mcr: Mean Crossing Rate (cruzamentos pela média)

3. FEATURES ESPECTRAIS (4 por sensor):
   - dominant_freq: Frequência dominante (FFT)
   - spectral_energy: Energia total do espectro (ENERGY)
   - spectral_entropy: Regularidade do espectro (Spectral Entropy)
   - spectral_centroid: Centro de massa do espectro

4. FEATURES MULTI-SENSOR (12 features adicionais do paper):
   - AI: Average Intensity (média do Movement Intensity)
   - VI: Variance Intensity (variância do Movement Intensity)
   - SMA: Normalized Signal Magnitude Area
   - EVA1, EVA2, EVA3: Eigenvalues das direções dominantes
   - CAGH: Correlation between Acceleration along Gravity and Heading
   - AVH: Averaged Velocity along Heading Direction
   - AVG: Averaged Velocity along Gravity Direction
   - ARATG: Averaged Rotation Angles related to Gravity Direction
   - AAE: Averaged Acceleration Energy
   - ARE: Averaged Rotation Energy

ABORDAGEM OTIMIZADA - FEATURES BASEADAS EM MÓDULOS:

Em vez de extrair features de cada eixo individual (acc_x, acc_y, acc_z),
extraímos features dos MÓDULOS dos sensores:
  - acc_module = sqrt(acc_x² + acc_y² + acc_z²)
  - gyro_module = sqrt(gyro_x² + gyro_y² + gyro_z²)
  - mag_module = sqrt(mag_x² + mag_y² + mag_z²)

JUSTIFICAÇÃO TÉCNICA:

1. INVARIÂNCIA ROTACIONAL:
   ✓ O módulo é invariante à rotação do referencial de coordenadas
   ✓ Atividades são classificadas pela MAGNITUDE do movimento, não pela direção
   ✓ Ex: "Stand" tem módulo baixo independentemente da orientação do dispositivo
   ✓ Ex: "Walk" tem padrão rítmico no módulo, não em eixos específicos

2. REDUÇÃO DE CORRELAÇÃO E REDUNDÂNCIA:
   ✓ Eixos x,y,z são altamente correlacionados (ρ>0.8 para muitas atividades)
   ✓ Usar eixos individuais = 9 eixos × 18 features = 162 features correlacionadas
   ✓ Usar módulos = 3 módulos × 18 features = 54 features independentes
   ✓ Redução de 66% da dimensionalidade sem perda significativa de informação

3. INTERPRETAÇÃO FÍSICA:
   ✓ Módulo = MAGNITUDE TOTAL da aceleração/rotação
   ✓ Representa intensidade do movimento, não direção específica
   ✓ Alinhado com conceitos biomecânicos de análise de movimento humano
   ✓ Mais robusto a pequenas variações na colocação do sensor

4. VALIDAÇÃO EMPÍRICA (ANOVA - Exercício 4.2):
   ✓ Teste estatístico confirma: módulos têm poder discriminante significativo
   ✓ p-values << 0.05 para a maioria das atividades
   ✓ Diferenças entre classes são estatisticamente significativas

5. VANTAGENS PRÁTICAS:
   ✓ Menos features = treinamento mais rápido de modelos
   ✓ Menos features = melhor generalização (menos overfitting)
   ✓ Computacionalmente eficiente para dispositivos embedded/móvel
   ✓ Mais interpretável para profissionais da área (fisiologistas, treinadores)

6. LIMITAÇÕES CONTROLADAS:
   ✗ Perde informação sobre direção específica do movimento
     → MAS: Atividades são definidas por padrão de movimento, não direção
   ✗ Requer verificação de robustez à orientação do dispositivo
     → MAS: Projeto já usa apenas pulso direito (orientação consistente)

CONCLUSÃO:
A abordagem com MÓDULOS é superior para classificação de ATIVIDADES porque:
1. Reduz correlação entre features (54 features vs 162 features correlacionadas)
2. Mantém invariância rotacional (propriedade desejável em HAR)
3. Tem validação estatística (ANOVA significativa)
4. Melhora eficiência computacional e generalização
5. É mais robusta a variações de colocação do sensor

VANTAGENS:
  ✓ Menos redundância (eixos x,y,z são correlacionados)
  ✓ Módulo representa magnitude/intensidade total do sensor
  ✓ Mais interpretável fisicamente
  ✓ Reduz dimensionalidade mantendo informação relevante
  ✓ Consistente com análises de outliers anteriores

TOTAL DE FEATURES:
- 3 módulos (acc_module, gyro_module, mag_module)
- 18 features por módulo (14 temporais + 4 espectrais)
- 12 features multi-sensor (baseadas em combinações de sensores)
- = 3×18 + 12 = 54 + 12 = 66 features por janela

DATASET FINAL:
- 27,845 janelas válidas (após descarte de 11.45%)
- Matriz [27845 × 66 features]
- Salvo em formato NumPy (.npz)

ANÁLISE DO FEATURE SET:

1. DIMENSÕES:
   - Compressão significativa dos dados brutos
   - Milhões de amostras → Milhares de janelas
   - 12 valores por amostra → 66 features por janela
   - 3 módulos × 18 features + 12 multi-sensor = 66 features

2. DISTRIBUIÇÃO:
   - Features com alta variabilidade são mais discriminantes
   - Correlações entre features indicam redundância
   - Algumas features podem ser removidas (seleção futura)

3. QUALIDADE:
   - Sem valores NaN ou Inf (dataset limpo)
   - Todas as atividades representadas
   - Distribuição balanceada por dispositivo

INTERPRETAÇÃO FÍSICA DAS FEATURES:

TEMPORAIS (14 features por módulo):
- mean/median: Valor central, distinguem atividades estáticas vs dinâmicas
- std/variance: Variabilidade do movimento
- min/max/range: Amplitude e extremos do movimento
- rms: Energia quadrática média do sinal
- mad: Desvio médio absoluto (averaged derivatives)
- iqr: Medida robusta de dispersão (interquartile range)
- skewness/kurtosis: Forma da distribuição (simetria e peakedness)
- zcr/mcr: Oscilações e mudanças de direção (passos, movimentos periódicos)

ESPECTRAIS (4 features por módulo):
- dominant_freq: Ritmo do movimento (andar, correr)
- spectral_energy: Intensidade geral da atividade
- spectral_entropy: Regularidade vs irregularidade
- spectral_centroid: Distribuição de frequências

MULTI-SENSOR (12 features):
- AI/VI: Intensidade e variabilidade do movimento total
- SMA: Energia normalizada de movimento
- EVA1/EVA2/EVA3: Direções principais de movimento
- CAGH: Correlação entre aceleração e rotação
- AVH/AVG: Velocidades em diferentes direções
- ARATG: Magnitude de rotação acumulada
- AAE/ARE: Energias de aceleração e rotação

APLICAÇÕES:

1. CLASSIFICAÇÃO SUPERVISIONADA:
   - Treinar modelos: SVM, Random Forest, Neural Networks
   - Features prontas para uso direto
   - Labels (atividades) disponíveis

2. REDUÇÃO DE DIMENSIONALIDADE:
   - PCA: Reduzir 66 features para dimensão menor
   - Feature Selection: Escolher features mais importantes
   - Remover features correlacionadas

3. ANÁLISE EXPLORATÓRIA:
   - Visualizar separabilidade das classes
   - Identificar features mais discriminantes
   - Comparar importância de sensores

PRÓXIMOS PASSOS SUGERIDOS:

1. Seleção de features (reduzir dimensionalidade)
2. Normalização/Padronização das features
3. Divisão em treino/validação/teste
4. Treino de classificadores
5. Avaliação de performance
6. Optimização de hiperparâmetros

================================================================================
EXERCÍCIO 4.3: IMPLEMENTAÇÃO DE PCA
================================================================================

OBJETIVO:
Desenvolver código necessário para implementar PCA (Principal Component Analysis)
do feature set extraído.

METODOLOGIA:

Principal Component Analysis (PCA) é uma técnica de redução de dimensionalidade
que transforma o conjunto de features em um novo espaço ortogonal onde:
- Os componentes principais são ordenados por variância explicada
- Máxima variância está no primeiro componente
- Cada componente é perpendicular aos anteriores
- O novo espaço é uma combinação linear das features originais

IMPLEMENTAÇÃO:

1. NORMALIZAÇÃO COM Z-SCORE:
   Z = (X - μ) / σ
   
   Necessária para:
   - Dar igual peso a todas as features
   - Evitar que features com maior escala dominem os componentes
   - Melhorar convergência e interpretabilidade
   
   Implementação:
   - StandardScaler (sklearn): transforma dados para média 0 e std 1
   - Aplicado a cada feature independentemente

2. APLICAÇÃO DE PCA:
   - Cálculo da matriz de covariância dos dados normalizados
   - Decomposição em eigenvalores e eigenvectors
   - Eigenvectors = direções dos componentes principais
   - Eigenvalores = variância explicada em cada direção
   - Transformação: X_pca = X_normalized × W
     (W é a matriz de eigenvectors, dimensão [n_features × n_components])

3. FUNÇÕES IMPLEMENTADAS:
   
   a) normalize_features_zscore(feature_matrix)
      - Normaliza features com Z-Score
      - Retorna: dados normalizados + objeto scaler
      - Scaler pode ser usado em novos dados
   
   b) apply_pca(X_normalized, n_components)
      - Aplica PCA ao conjunto normalizado
      - Retorna: objeto PCA + dados transformados
   
   c) analyze_variance_explained(pca)
      - Analisa variância explicada por cada componente
      - Calcula percentis: 75%, 80%, 90%, 95%
      - Determina número de componentes para cada threshold
   
   d) create_variance_plot(pca, variance_info, output_dir)
      - Cria gráfico de variância acumulada
      - Mostra ponto crítico para 75%
      - Facilita decisão sobre redução de dimensionalidade

VANTAGENS DA IMPLEMENTAÇÃO:

- Modular: Cada função tem responsabilidade bem definida
- Reutilizável: Normalização e PCA podem ser aplicados a novos dados
- Rastreável: Scaler e PCA são salvos para transformações futuras
- Eficiente: Usa sklearn (implementação otimizada)

================================================================================
EXERCÍCIO 4.4: ANÁLISE DE VARIÂNCIA E COMPRESSÃO
================================================================================

OBJETIVO:
1. Determinar importância de cada vetor principal na explicação da variabilidade
2. Calcular quantas dimensões explicam 75% da variância
3. Demonstrar compressão de features
4. Analisar vantagens e limitações

RESULTADOS ESPERADOS:

Baseado na estrutura do problema (66 features originais - módulos de sensores):

VARIÂNCIA ACUMULADA:
- Primeiros 3-5 componentes: ~70-80% da variância
- Primeiros 10 componentes: ~85-90% da variância
- Primeiros 15 componentes: ~90-95% da variância
- Primeiros 30 componentes: ~98% da variância

INTERPRETAÇÃO:
A distribuição típica em PCA segue a regra de Pareto:
- Poucos componentes explicam a maioria da variância
- Muitos componentes contribuem pouco individualmente
- Trade-off entre redução de dimensionalidade e perda de informação

RESULTADOS REAIS:

Após aplicação de PCA ao dataset de features extraído (27,845 janelas × 66 features):

VARIÂNCIA ACUMULADA (RESULTADOS EXPERIMENTAIS):
- 5 componentes: 78.29% da variância (compressão: 92.4%)
- 6 componentes: 81.27% da variância (compressão: 90.9%)
- 11 componentes: 90.30% da variância (compressão: 83.3%)
- 17 componentes: 95.52% da variância (compressão: 74.2%)
- 66 componentes: 100.00% da variância (sem compressão)

RESPOSTA À QUESTÃO DO EXERCÍCIO:
✓ "Quantas dimensões deverá usar para explicar 75% do feature set?"
✓ RESPOSTA: 5 componentes principais
✓ Com apenas 5 dimensões (7.6% dos originais) explicamos 78.29% da variância!
✓ Redução de dimensionalidade: de 66 para 5 features (92.4% de compressão)
✓ Resultado excelente: muito melhor que abordagem por eixos individuais

COMPARAÇÃO COM ABORDAGEM ANTERIOR (174 FEATURES):
- Abordagem por eixos: 16 componentes para 76% de variância (90.8% compressão)
- Abordagem por módulos: 5 componentes para 78% de variância (92.4% compressão)
- Melhoria: 3.2× menos componentes, compressão 1.6pp superior
- Conclusão: Módulos reduzem redundância de forma muito mais eficaz

ANÁLISE DETALHADA:

1. IMPORTÂNCIA DOS VETORES PRINCIPAIS:

   Definição:
   - Variância explicada do componente i = λ_i / Σλ_j
   - Onde λ_i são os eigenvalues (ordenados decrescente)
   
   Interpretação:
   - Componente 1: ~49.88% da variância (dominante)
   - Componente 2: ~11.45% da variância
   - Componente 3: ~8.94% da variância
   - Componente 4: ~4.24% da variância
   - Componente 5: ~3.77% da variância
   - Componentes restantes: decaem progressivamente
   
   Significado físico:
   - Componente 1 captura o padrão dominante de diferenciação
   - Componentes posteriores capturam nuances e padrões secundários
   - Redução: manter ~7.6% de componentes retém 78% de informação

2. DETERMINAÇÃO DE DIMENSÕES PARA 75% DE VARIÂNCIA:

   Método:
   - Somar eigenvalues em ordem decrescente
   - Encontrar número mínimo onde Σ(λ_i) / Σ(λ_todos) ≥ 0.75
   
   RESULTADO REAL:
   - Número de componentes: 5 (7.6% dos 66 originais)
   - Taxa de compressão: 92.4% (excelente!)
   - Redução de dimensionalidade: de 66 para 5 features
   - Variância explicada: 78.29% (acima de 75%)

   INTERPRETAÇÃO DO RESULTADO:
   O dataset baseado em módulos tem estrutura muito concentrada.
   O componente 1 sozinho explica 49.88% da variância, indicando que
   há um padrão dominante muito claro de diferenciação entre atividades.
   A abordagem por módulos elimina redundância de forma muito eficaz.

3. BENEFÍCIOS DA COMPRESSÃO:

   a) Redução computacional:
      - Menos features → treino muito mais rápido (13× menos dimensões)
      - Menos memória para armazenar dados
      - Menos parâmetros em modelos de ML
   
   b) Melhoria de generalização:
      - Reduz overfitting em dados de treino
      - Features ruidosas e redundantes são eliminadas
      - Melhora performance em dados de teste
   
   c) Interpretabilidade:
      - Espaço reduzido é mais fácil de visualizar (2D ou 3D)
      - Componentes principais identificam padrões principais
      - Facilita análise exploratória

================================================================================
EXERCÍCIO 4.4.1: OBTENÇÃO DE FEATURES COMPRIMIDAS
================================================================================

OBJETIVO:
Indicar como obter features comprimidas neste novo espaço e exemplificar para
um instante específico.

MÉTODO:

1. TREINAR PCA NO CONJUNTO COMPLETO:
   
   X_original ∈ ℝ^(n × 66)      # n amostras, 66 features
   X_normalized ∈ ℝ^(n × 66)    # Após normalização Z-Score
   X_compressed ∈ ℝ^(n × 5)     # Após PCA com 5 componentes (78% variância)

2. TRANSFORMAÇÃO MATEMÁTICA:
   
   Para uma amostra individual:
   
   Passo 1 - Normalização:
   x_normalized = (x - μ) / σ
   
   Passo 2 - Projeção no espaço PCA:
   x_compressed = x_normalized × W
   
   Onde W é a matriz de eigenvectors [66 × 5]
   
   Resultado: x_compressed é um vetor [5 × 1] com os componentes principais

3. IMPLEMENTAÇÃO EM CÓDIGO:

   def get_compressed_features_for_sample(sample, pca, scaler):
       # Normalizar amostra
       sample_normalized = scaler.transform(sample.reshape(1, -1))
       # Aplicar PCA
       sample_compressed = pca.transform(sample_normalized)
       return sample_compressed[0]  # Retorna array [5]

EXEMPLO PRÁTICO:

Amostra: Índice 0 (primeira janela de 5 segundos)
Atividade: ID 1 (primeira atividade registada)

Features Originais (66):
- 3 módulos × 18 features temporais/espectrais + 12 multi-sensor
- Primeiros 10 valores (normalizados): [-0.649, 0.000, -0.918, 0.068, -0.896, -0.804, 0.411, -0.678, -0.454, 0.859]
- Valores cobrem o intervalo [μ-5σ, μ+5σ]

Features Comprimidas (5):
- Componente 1: -4.762  (direção dominante de variação)
- Componente 2: -0.068  (segundo padrão mais importante)
- Componente 3: 0.592   (terceiro padrão)
- Componente 4: 0.995   (quarto padrão)
- Componente 5: -0.889  (quinto padrão)

Interpretação:
- O valor -4.762 no componente 1 indica que esta amostra está -4.762 desvios
  padrão abaixo da média na direção do componente 1
- Componentes posteriores têm magnitudes menores (menos importantes)
- A compressão retém 78% da informação em apenas 7.6% do espaço

VANTAGENS PARA CLASSIFICAÇÃO:
- 5 features em vez de 66: 13× mais rápido
- Espaço menos ruidoso: melhor separabilidade de classes
- Menos correlação entre features: evita multicolinearidade
- Redução dramática de dimensionalidade sem perda significativa de informação

================================================================================
EXERCÍCIO 4.4.2: VANTAGENS E LIMITAÇÕES
================================================================================

VANTAGENS:

1. REDUÇÃO DE DIMENSIONALIDADE:
   ✓ Espaço 23% do tamanho original
   ✓ Treino 2-3× mais rápido
   ✓ Menos memória necessária
   ✓ Menos parâmetros em modelos

2. MELHORIA DE GENERALIZAÇÃO:
   ✓ Remove ruído e features redundantes
   ✓ Reduz overfitting
   ✓ Melhora performance em dados de teste
   ✓ Mais robusto a dados fora-da-distribuição

3. VISUALIZAÇÃO:
   ✓ Primeiros 2-3 componentes podem ser visualizados em 2D/3D
   ✓ Facilita análise exploratória
   ✓ Identifica clusters e separabilidade de classes
   ✓ Detecta anomalias visuais

4. INTERPRETABILIDADE:
   ✓ Componentes principais têm significado físico
   ✓ Pode-se analisar que sensores contribuem mais
   ✓ Identifica direções de máxima variância
   ✓ Facilita comunicação de resultados

5. PROPRIEDADES MATEMÁTICAS:
   ✓ Componentes não correlacionados (ortogonais)
   ✓ Elimina multicolinearidade
   ✓ Preserva estrutura de distâncias (aproximadamente)
   ✓ Teoricamente ótimo para redução linear

LIMITAÇÕES:

1. PERDA DE INFORMAÇÃO:
   ✗ 25% da variância é descartada
   ✗ Possível perda de discriminabilidade entre classes
   ✗ Padrões minoritários podem ser perdidos
   ✗ Irreversível: não é possível recuperar features originais com precisão

2. INTERPRETABILIDADE DOS COMPONENTES:
   ✗ Componentes são combinações lineares de todas as features
   ✗ Difícil associar componente a um sensor específico
   ✗ Componentes não têm unidades físicas claras
   ✗ Requer análise de loadings para compreender composição

3. LINEARIDADE:
   ✗ PCA é linear: não captura relações não-lineares entre features
   ✗ Padrões não-lineares podem não ser bem representados
   ✗ Para dados com estrutura não-linear (manifolds), alternativas como kernel-PCA ou autoencoders são melhores
   ✗ Relações complexas entre sensores podem ser ignoradas

4. SENSIBILIDADE À NORMALIZAÇÃO:
   ✗ Resultados dependem do método de normalização
   ✗ Z-Score vs Min-Max vs Log podem dar componentes diferentes
   ✗ Features outliers influenciam o cálculo
   ✗ Necessário tratar outliers antes de aplicar PCA

5. SELEÇÃO DE NÚMERO DE COMPONENTES:
   ✗ Limiares como 75% são arbitrários
   ✗ Escolha diferente pode impactar performance
   ✗ Sem validação cruzada, pode ser subótima
   ✗ Necessário experimentação e fine-tuning

6. APLICAÇÃO A NOVOS DADOS:
   ✗ Deve-se usar o mesmo scaler e PCA treinados
   ✗ Dados novos em distribuição diferente podem ter problemas
   ✗ Necessário armazenar scaler e componentes PCA
   ✗ Ajustes requeridos se distribuição muda

7. COMPARAÇÃO COM ALTERNATIVAS:
   
   PCA vs Feature Selection:
   - PCA: cria novas features (combinações), retém estrutura global
   - Feature Selection: mantém features originais, mais interpretável
   
   PCA vs Autoencoders:
   - PCA: linear, rápido, teoricamente fundamentado
   - Autoencoders: não-linear, mais flexível, mais lento
   
   PCA vs t-SNE / UMAP:
   - PCA: preserva distâncias globais, linear
   - t-SNE/UMAP: melhor visualização, preservam distâncias locais

RECOMENDAÇÕES PRÁTICAS:

1. SEMPRE verificar variância explicada antes de escolher n_componentes
2. USAR validação cruzada para otimizar número de componentes
3. CONSIDERAR alternativas (feature selection, kernel-PCA) para dados não-lineares
4. NORMALIZAR antes de PCA (especialmente com features de escalas diferentes)
5. ARMAZENAR scaler e PCA para aplicação em dados futuros
6. VISUALIZAR primeiros 2-3 componentes para exploração
7. COMPARAR com baseline sem PCA para validar que ajuda

CONCLUSÃO SOBRE PCA:

PCA é uma ferramenta poderosa para redução de dimensionalidade quando:
- Dados têm estrutura linear
- Há redundância e correlação entre features
- Visualização em espaço reduzido é desejável
- Interpretabilidade de componentes é possível

Porém, não é universal. Em alguns casos:
- Feature Selection é melhor (mantém interpretabilidade)
- Métodos não-lineares são superiores (dados complexos)
- Sem compressão é mais simples (se dados couber em memória)
- Domain-specific engineering é mais eficaz (se padrões são conhecidos)

Para classificação de atividades:
- PCA é recomendado como primeiro passo de pré-processamento
- Redução para 75% de variância é geralmente segura
- Combinado com feature selection pode otimizar ainda mais
- Validação experimental é crucial para confirmar benefícios

================================================================================
CONCLUSÕES GERAIS DO PROJETO
================================================================================

1. DETECÇÃO DE OUTLIERS:
   - Múltiplos métodos implementados (IQR, Z-Score, K-Means, DBSCAN)
   - Cada método tem vantagens específicas
   - Recomenda-se uso complementar de vários métodos

2. ANÁLISE ESTATÍSTICA:
   - Módulos dos sensores são altamente discriminantes
   - Diferenças estatisticamente significativas entre atividades
   - Justifica uso destes sensores para classificação

3. EXTRAÇÃO DE FEATURES:
   - 66 features extraídas com sucesso (baseadas em módulos de sensores)
   - Dataset comprimido e pronto para Machine Learning
   - Combina informação temporal e espectral
   - Abordagem por módulos reduz redundância e melhora interpretabilidade

4. REDUÇÃO DE DIMENSIONALIDADE (PCA):
   - PCA reduz dimensionalidade de 66 para 5 features (92.4% de compressão)
   - Retém 78.29% da variância com apenas 5 componentes principais
   - Resultado excelente: 13× redução dimensional mantendo alta variância
   - Melhora generalização e acelera treino significativamente
   - Trade-off favorável: perda de 22% de informação vs. benefícios computacionais (13× redução)
   - Comparação com abordagem anterior (174 features): PCA mais eficiente com módulos

5. SELEÇÃO DE FEATURES (FISHER SCORE E ReliefF):
   - Dois métodos complementares de seleção de features implementados
   - Fisher Score identifica features discriminantes baseado em médias/variâncias
   - ReliefF identifica features relevantes baseado em instâncias vizinhas
   - Top-10 features identificadas por cada método
   - Análise comparativa revela concordâncias e diferenças entre métodos
   - Métodos complementares oferecem perspectiva global (Fisher) e local (ReliefF)

6. QUALIDADE DOS DADOS:
   - Dados limpos (sem NaN ou Inf)
   - Distribuição adequada de amostras
   - Todas as atividades e dispositivos representados
   - 27,845 janelas válidas (taxa de descarte: 11.45%)

7. APLICABILIDADE:
   - Pipeline completo: pré-processamento → features → normalização → PCA → seleção
   - Pronto para classificação (fase de ML)
   - Metodologia replicável para novos dados
   - Métodos de seleção de features facilitam interpretabilidade e eficiência

COMPARAÇÃO DE ABORDAGENS:

Abordagem por EIXOS (anterior):
  • 9 eixos individuais × 18 features + 12 multi-sensor = 174 features
  • PCA: 16 componentes para 76% de variância
  • Compressão: 90.8%
  • Redundância elevada (eixos correlacionados)

Abordagem por MÓDULOS (atual, otimizada):
  • 3 módulos × 18 features + 12 multi-sensor = 66 features
  • PCA: 5 componentes para 78% de variância
  • Compressão: 92.4%
  • Menos redundância, mais interpretável
  • ✓ RECOMENDADA

================================================================================
EXERCÍCIO 4.5: IMPLEMENTAÇÃO DO FISHER SCORE E ReliefF
================================================================================

OBJETIVO:
Desenvolver código necessário para implementar Fisher Score e ReliefF.
Ambos são métodos supervisionados de seleção de features, mas com abordagens distintas.

METODOLOGIA:

1. FISHER SCORE:

Conceito:
- Método estatístico que avalia poder discriminante de cada feature
- Baseado na razão entre variabilidade inter-classe e intra-classe
- Similar à ideia do teste F da ANOVA

Fórmula:
Fisher_Score(j) = Σ_c n_c * (μ_cj - μ_j)² / Σ_c n_c * σ²_cj

Onde:
- j: índice da feature
- c: índice da classe
- n_c: número de amostras da classe c
- μ_cj: média da feature j na classe c
- μ_j: média global da feature j
- σ²_cj: variância da feature j na classe c

Interpretação:
- Numerador: Variabilidade ENTRE classes (quanto mais diferente, melhor)
- Denominador: Variabilidade DENTRO de classes (quanto menos disperso, melhor)
- Score alto = feature discrimina bem entre classes
- Score baixo = feature não discrimina bem

Vantagens:
✓ Rápido de calcular (complexidade O(n × m), n=amostras, m=features)
✓ Teoricamente fundamentado (estatística clássica)
✓ Não requer parâmetros ou tuning
✓ Interpretável (razão de variâncias)
✓ Funciona bem com features gaussianas
✓ Escala linearmente com número de features

Desvantagens:
✗ Assume distribuições gaussianas (menos robusto a não-normalidade)
✗ Avaliação global (não considera relações locais)
✗ Univariado (avalia features independentemente)
✗ Não captura interações entre features
✗ Sensível a outliers (usa média e variância)

2. ReliefF:

Conceito:
- Método baseado em instâncias (instance-based)
- Avalia features pela capacidade de distinguir instâncias vizinhas
- Versão melhorada do RELIEF original para multi-classes

Algoritmo:
Para cada instância aleatória:
  1. Encontra k nearest hits (mesma classe, instâncias mais próximas)
  2. Encontra k nearest misses (classe diferente, instâncias mais próximas)
  3. Atualiza pesos:
     - Penaliza se feature difere entre instância e hits (mesma classe)
     - Recompensa se feature difere entre instância e misses (classes diferentes)

Fórmula de atualização:
W[j] = W[j] - diff(instance, hit)[j] / (m × k) 
            + Σ_c P(c) × diff(instance, miss_c)[j] / (m × k)

Onde:
- W[j]: peso da feature j
- m: número de iterações (amostras)
- k: número de vizinhos
- P(c): probabilidade da classe c
- diff: diferença normalizada entre instâncias

Interpretação:
- Peso positivo alto = feature separa bem instâncias de classes diferentes
- Peso negativo = feature confunde instâncias (não discriminante)
- Baseado em padrões locais (vizinhos próximos)

Vantagens:
✓ Não assume distribuições (não-paramétrico)
✓ Captura relações locais e não-lineares
✓ Robusto a outliers (usa vizinhos)
✓ Multi-variado (considera contexto das outras features na distância)
✓ Detecta interações entre features
✓ Funciona bem com dados de qualquer distribuição

Desvantagens:
✗ Mais lento (complexidade O(m × n²), requer cálculo de distâncias)
✗ Requer escolha de parâmetros (k vizinhos, m iterações)
✗ Resultados variam com amostragem aleatória
✗ Difícil interpretar pesos negativos
✗ Requer mais dados para estimativa estável
✗ Computacionalmente custoso para datasets grandes

IMPLEMENTAÇÃO:

1. Fisher Score:
   - Implementação direta da fórmula matemática
   - Usa NumPy para cálculos vetorizados
   - Evita divisão por zero (variância muito pequena)
   - Retorna array de scores [n_features]

2. ReliefF:
   - Implementação simplificada do algoritmo
   - Usa sklearn.neighbors para cálculo eficiente de vizinhos
   - Normalização implícita via distâncias euclidianas
   - Amostragem aleatória para eficiência
   - Retorna array de pesos [n_features]

PARÂMETROS UTILIZADOS:

Fisher Score:
- Nenhum parâmetro necessário

ReliefF:
- k (n_neighbors): 10 vizinhos
  → Balanceia entre precisão local e estabilidade
  → Valores típicos: 5-20
  
- m (n_samples): 100 iterações
  → Amostra reduzida para eficiência computacional
  → Dataset grande (27,845 janelas), amostra suficiente
  → Valores típicos: 50-500

COMPLEXIDADE COMPUTACIONAL:

Fisher Score:
- Tempo: O(n × m × c)
  → n: número de amostras (27,845)
  → m: número de features (66)
  → c: número de classes (atividades)
  → Muito rápido: ~segundos

ReliefF:
- Tempo: O(m × n × d)
  → m: iterações (100 amostras)
  → n: tamanho do dataset (27,845)
  → d: número de features (66) para cálculo de distâncias
  → Moderado: ~minutos (por isso usamos amostragem)

CONSIDERAÇÕES DE IMPLEMENTAÇÃO:

1. Normalização:
   - Fisher Score: Não requer normalização (trabalha com variâncias relativas)
   - ReliefF: Features já normalizadas no dataset (Z-Score aplicado em 4.3)

2. Eficiência:
   - Fisher Score: Processa todas as 27,845 janelas
   - ReliefF: Amostra 100 janelas aleatórias (0.36% dos dados)
     → Suficiente para estimar pesos de forma confiável
     → Trade-off entre precisão e tempo computacional

3. Estabilidade:
   - Fisher Score: Determinístico (sempre mesmo resultado)
   - ReliefF: Estocástico (varia com seed aleatória)
     → Para reprodutibilidade, fixar np.random.seed()

================================================================================
EXERCÍCIO 4.6: IDENTIFICAÇÃO E COMPARAÇÃO DAS TOP-10 FEATURES
================================================================================

OBJETIVO:
Identificar as 10 melhores features de acordo com Fisher Score e ReliefF
e comparar os resultados obtidos por cada método.

RESULTADOS ESPERADOS:

Baseado na estrutura do dataset (66 features baseadas em módulos):

CATEGORIAS DE FEATURES:
- Temporais dos módulos: mean, std, max, min, range, rms, mad, iqr, skewness, 
  kurtosis, zcr, mcr (14 × 3 módulos = 42 features)
- Espectrais dos módulos: dominant_freq, spectral_energy, spectral_entropy, 
  spectral_centroid (4 × 3 módulos = 12 features)
- Multi-sensor: AI, VI, SMA, EVA1-3, CAGH, AVH, AVG, ARATG, AAE, ARE (12 features)

HIPÓTESES SOBRE TOP FEATURES:

1. Features temporais de energia/magnitude:
   - acc_module_mean, acc_module_std, acc_module_rms
   → Capturam intensidade do movimento (crucial para separar atividades)

2. Features multi-sensor de movimento:
   - AI (Average Intensity), VI (Variance Intensity)
   - AAE (Averaged Acceleration Energy), ARE (Averaged Rotation Energy)
   → Combinam informação de múltiplos sensores

3. Features espectrais:
   - dominant_freq: frequência dominante (ritmo de movimentos periódicos)
   - spectral_energy: energia total (intensidade da atividade)

4. Features de variabilidade:
   - std, variance, range, iqr
   → Distinguem atividades estáticas de dinâmicas

ANÁLISE COMPARATIVA ESPERADA:

FISHER SCORE:
- Deve priorizar features com grande separação entre médias de classes
- Esperado: features de energia, magnitude e estatísticas robustas
- Menos sensível a padrões locais

ReliefF:
- Deve priorizar features que distinguem vizinhos de classes diferentes
- Esperado: features que capturam transições e nuances
- Mais sensível a padrões locais e interações

CONCORDÂNCIA ESPERADA:
- Alta concordância (70-80%) nas features mais óbvias (energia, magnitude)
- Divergência em features mais sutis (momentos estatísticos, espectrais)
- Fisher pode priorizar features globalmente discriminantes
- ReliefF pode priorizar features localmente informativas

MÉTRICAS DE COMPARAÇÃO:

1. Features comuns:
   - Número de features que aparecem no top-10 de ambos
   - Esperado: 6-8 features comuns

2. Taxa de concordância:
   - Proporção de features comuns / total
   - Esperado: 60-80%

3. Diferença média de posição:
   - Para features comuns, diferença média de ranking
   - Esperado: 2-3 posições

4. Análise qualitativa:
   - Categorias de features prioritizadas por cada método
   - Tipo de informação capturada (global vs. local)

INTERPRETAÇÃO DOS RESULTADOS:

1. CONCORDÂNCIA ALTA (>70%):
   → Features universalmente discriminantes
   → Padrões claros e robustos
   → Menos necessidade de feature engineering adicional

2. CONCORDÂNCIA MODERADA (50-70%):
   → Métodos capturam aspectos diferentes
   → Útil combinar features de ambos
   → Indica riqueza de informação no dataset

3. CONCORDÂNCIA BAIXA (<50%):
   → Dataset complexo com múltiplos padrões
   → Necessário análise mais profunda
   → Considerar ensemble de métodos de seleção

APLICAÇÕES PRÁTICAS:

1. Feature Selection para Classificação:
   - Usar união das top-10 de ambos (até 20 features)
   - Ou interseção (features mais robustas)
   - Reduz de 66 para 10-20 features mantendo discriminabilidade

2. Feature Engineering:
   - Features não selecionadas podem ser descartadas
   - Focar esforços em melhorar features selecionadas
   - Criar features derivadas das selecionadas

3. Interpretabilidade:
   - Features selecionadas indicam quais aspectos do movimento são importantes
   - Permite insights sobre diferenças entre atividades
   - Facilita explicação de modelos de ML

4. Validação Cruzada:
   - Comparar seleção em diferentes folds
   - Verificar estabilidade da seleção
   - Features consistentes são mais confiáveis

OBSERVAÇÕES IMPORTANTES:

1. Complementaridade:
   - Fisher Score e ReliefF são complementares
   - Fisher: visão estatística global
   - ReliefF: visão baseada em instâncias locais
   - Usar ambos dá perspectiva mais completa

2. Trade-offs:
   - Fisher Score: rápido mas assume normalidade
   - ReliefF: robusto mas computacionalmente caro
   - Escolha depende de dataset e recursos

3. Limitações:
   - Ambos são univariados/filter methods
   - Não consideram redundância entre features selecionadas
   - Para seleção ótima, considerar wrapper methods (RFE, SFS)
   - Ou embedded methods (Lasso, Random Forest importances)

4. Próximos Passos:
   - Treinar classificadores com features selecionadas
   - Comparar performance com todas as 66 features
   - Validar se redução de dimensionalidade mantém acurácia
   - Considerar métodos híbridos (filter + wrapper)

================================================================================
COMPARAÇÃO DETALHADA: FISHER SCORE vs. ReliefF
================================================================================

DIMENSÃO DE ANÁLISE:

1. ABORDAGEM METODOLÓGICA:

Fisher Score:
- Estatístico, paramétrico
- Baseado em momentos (média, variância)
- Avaliação global de toda a distribuição
- Teoria: razão de variâncias (como ANOVA)

ReliefF:
- Baseado em instâncias (instance-based)
- Baseado em similaridades (distâncias)
- Avaliação local (vizinhanças)
- Teoria: diferenças entre vizinhos

2. SUPOSIÇÕES:

Fisher Score:
- Assume distribuições aproximadamente gaussianas
- Sensível a outliers (usa média)
- Features avaliadas independentemente
- Relações lineares entre features e classes

ReliefF:
- Não assume distribuições específicas (distribution-free)
- Robusto a outliers (usa vizinhos)
- Features avaliadas em contexto (distâncias multi-dimensionais)
- Captura relações não-lineares

3. TIPO DE INFORMAÇÃO CAPTURADA:

Fisher Score prioriza:
- Separação de médias entre classes
- Consistência intra-classe (baixa variância)
- Padrões globais
- Features com valores distintivos por classe

ReliefF prioriza:
- Similaridade intra-classe
- Dissimilaridade inter-classe
- Padrões locais (fronteiras de decisão)
- Features que separam vizinhos de classes diferentes

4. CENÁRIOS ONDE CADA MÉTODO BRILHA:

Fisher Score é melhor quando:
✓ Dados seguem distribuições normais
✓ Classes bem separadas globalmente
✓ Features com médias muito diferentes entre classes
✓ Necessário método rápido e interpretável
✓ Dataset grande com features gaussianas

ReliefF é melhor quando:
✓ Dados não-gaussianos ou multimodais
✓ Classes com fronteiras complexas
✓ Interações entre features são importantes
✓ Necessário método robusto a outliers
✓ Padrões locais/não-lineares existem

5. LIMITAÇÕES COMPLEMENTARES:

Fisher Score pode falhar se:
✗ Classes têm mesma média mas distribuições diferentes
✗ Separabilidade é não-linear
✗ Outliers distorcem média e variância
✗ Interações entre features são cruciais

ReliefF pode falhar se:
✗ Dataset é muito pequeno (instável)
✗ Ruído afeta cálculo de vizinhos
✗ Escolha inadequada de k (vizinhos)
✗ Custo computacional é proibitivo

6. INTERPRETAÇÃO DE SCORES:

Fisher Score:
- Score absoluto interpretável (razão de variâncias)
- Comparação direta entre features
- Threshold natural (score > 1 significa mais variância entre classes)

ReliefF:
- Pesos relativos (não absolutos)
- Dependem de escala e amostragem
- Comparação interna ao dataset
- Sem threshold natural claro

7. RECOMENDAÇÕES DE USO:

Usar FISHER SCORE se:
→ Exploração inicial rápida
→ Features têm distribuições normais
→ Interpretabilidade é crítica
→ Recursos computacionais limitados

Usar ReliefF se:
→ Dados complexos/não-lineares
→ Robustez é mais importante que velocidade
→ Interações entre features são suspeitas
→ Recursos computacionais disponíveis

Usar AMBOS se:
→ Análise completa é necessária
→ Combinar perspectivas global e local
→ Validar seleção por diferentes critérios
→ Identificar features consensualmente importantes

CONCLUSÃO:

Fisher Score e ReliefF são métodos complementares que oferecem perspectivas
diferentes sobre a importância das features. A concordância entre ambos indica
features robustamente discriminantes, enquanto divergências revelam aspectos
específicos de cada abordagem (global vs. local). O ideal é usar ambos e
analisar tanto features comuns quanto únicas de cada método para ter uma
compreensão completa do poder discriminante do feature set.

================================================================================
EXERCÍCIO 4.6.1: EXTRAÇÃO DE FEATURES SELECIONADAS
================================================================================

OBJETIVO:
Demonstrar como extrair apenas as features selecionadas pelos métodos de 
seleção (Fisher Score e ReliefF) a partir da matriz completa de features.

METODOLOGIA:

1. IDENTIFICAÇÃO DAS FEATURES SELECIONADAS:
   - Obter top-10 features do Fisher Score
   - Obter top-10 features do ReliefF
   - Calcular união de ambas (features únicas entre os dois métodos)

2. EXTRAÇÃO DAS FEATURES:
   
   Dado:
   - X: matriz original [n_samples × n_features] = [27,845 × 66]
   - feature_names: lista com 66 nomes de features
   - selected_features: lista com nomes das features selecionadas
   
   Processo:
   ```python
   # 1. Encontrar índices das features selecionadas
   selected_indices = [i for i, name in enumerate(feature_names) 
                      if name in selected_features]
   
   # 2. Extrair colunas correspondentes
   X_selected = X[:, selected_indices]
   ```
   
   Resultado:
   - X_selected: matriz reduzida [n_samples × n_selected]
   - Exemplo: [27,845 × 66] → [27,845 × 14] (união Fisher + ReliefF)

3. EXEMPLO PARA UM INSTANTE ESPECÍFICO:
   
   Para instante i (exemplo: i=12,500):
   
   a) Vetor original (66 features):
      sample_original = X[12500, :]  # shape: (66,)
   
   b) Vetor com features Fisher (10 features):
      sample_fisher = X_selected_fisher[12500, :]  # shape: (10,)
   
   c) Vetor com features ReliefF (10 features):
      sample_relieff = X_selected_relieff[12500, :]  # shape: (10,)
   
   d) Vetor com união (14 features típicas):
      sample_union = X_selected_union[12500, :]  # shape: (14,)

4. COMPRESSÃO ALCANÇADA:
   
   - Dimensionalidade original: 66 features
   - Após seleção Fisher: 10 features (84.8% compressão)
   - Após seleção ReliefF: 10 features (84.8% compressão)
   - União de ambos: ~14 features (78.8% compressão)
   
   Redução típica: 66 → 14 features mantendo as mais discriminantes

APLICAÇÕES PRÁTICAS:

1. TREINAMENTO DE MODELOS:
   ```python
   # Em vez de treinar com todas as 66 features:
   modelo.fit(X_completo, y)
   
   # Treinar apenas com features selecionadas:
   modelo.fit(X_selected, y)
   ```
   
   Vantagens:
   ✓ Treinamento 4-5× mais rápido
   ✓ Menor risco de overfitting
   ✓ Modelos mais interpretáveis
   ✓ Melhor generalização

2. INFERÊNCIA EM TEMPO REAL:
   ```python
   # Para nova amostra com 66 features:
   nova_amostra_completa = extrair_features(janela_sensores)
   
   # Extrair apenas features selecionadas:
   nova_amostra_selected = nova_amostra_completa[selected_indices]
   
   # Classificar:
   predicao = modelo.predict(nova_amostra_selected)
   ```
   
   Vantagens:
   ✓ Processamento mais rápido (menos features)
   ✓ Menor consumo de memória
   ✓ Viável para dispositivos embedded/móveis

3. ANÁLISE E VISUALIZAÇÃO:
   - Reduzir dimensionalidade facilita visualização
   - Focar em features mais importantes para interpretação
   - Identificar padrões com menos ruído

VALIDAÇÃO:

Para garantir que a extração está correta:
1. Verificar que selected_indices contém índices válidos (0 ≤ i < 66)
2. Confirmar que X_selected.shape[1] == len(selected_features)
3. Validar que valores correspondem: X_selected[i,j] == X[i, selected_indices[j]]

CONSIDERAÇÕES:

1. ORDEM DAS FEATURES:
   - A ordem na matriz selecionada segue a ordem dos índices
   - Pode-se reordenar por importância (ranking)

2. NORMALIZAÇÃO:
   - Se X foi normalizado, X_selected herda a normalização
   - Importante manter mesmo scaler para novas amostras

3. COMPATIBILIDADE:
   - Modelo treinado com X_selected só aceita features selecionadas
   - Pipeline de inferência deve aplicar mesma seleção

================================================================================
EXERCÍCIO 4.6.2: VANTAGENS E LIMITAÇÕES DA SELEÇÃO DE FEATURES
================================================================================

OBJETIVO:
Analisar criticamente as vantagens e limitações da abordagem de seleção de
features usando Fisher Score e ReliefF.

VANTAGENS:

1. REDUÇÃO DE DIMENSIONALIDADE:
   ✓ Compressão: 66 → 10-14 features (78-85% redução)
   ✓ Menor complexidade computacional em treinamento
   ✓ Menor uso de memória em inferência
   ✓ Viabiliza deployment em dispositivos com recursos limitados

2. MELHORIA DA GENERALIZAÇÃO:
   ✓ Menos features = menor risco de overfitting
   ✓ Remove features redundantes ou irrelevantes
   ✓ Foco em features com maior poder discriminante
   ✓ Modelos mais robustos a ruído

3. INTERPRETABILIDADE:
   ✓ Identificar quais características são mais importantes
   ✓ Compreender padrões que diferenciam atividades
   ✓ Validar intuições biomecânicas (ex: gyro_module_mean para rotações)
   ✓ Facilita explicação do modelo a stakeholders

4. EFICIÊNCIA COMPUTACIONAL:
   ✓ Treinamento 4-5× mais rápido com menos features
   ✓ Inferência mais rápida (crítico para tempo real)
   ✓ Menor custo de armazenamento de dados
   ✓ Viável para edge computing / dispositivos móveis

5. VALIDAÇÃO CRUZADA ENTRE MÉTODOS:
   ✓ Features comuns a Fisher e ReliefF (60% concordância típica)
   ✓ Alta confiança em features consensuais
   ✓ Perspectivas global (Fisher) e local (ReliefF)
   ✓ Robustez por triangulação de métodos

6. PREVENÇÃO DE CURSE OF DIMENSIONALITY:
   ✓ Menos dimensões = menos amostras necessárias
   ✓ Densidade amostral maior no espaço reduzido
   ✓ Melhor estimação de fronteiras de decisão
   ✓ Evita esparsidade em alta dimensionalidade

LIMITAÇÕES:

1. PERDA DE INFORMAÇÃO:
   ✗ Features descartadas podem conter informação útil
   ✗ Risco de remover features importantes para casos raros
   ✗ Possível perda de padrões sutis em features menos discriminantes
   ✗ Interações complexas entre features podem ser perdidas

2. DEPENDÊNCIA DO MÉTODO DE SELEÇÃO:
   ✗ Fisher Score assume separabilidade linear
   ✗ ReliefF sensível à escolha de k (vizinhos) e amostragem
   ✗ Diferentes métodos produzem diferentes subconjuntos
   ✗ Não há "verdade absoluta" sobre quais features são "melhores"

3. VALIDAÇÃO REQUERIDA:
   ✗ Necessário validar que performance não degrada
   ✗ Teste em conjunto de validação independente obrigatório
   ✗ Possível que 66 features originais sejam melhores para alguns casos
   ✗ Trade-off entre simplicidade e performance deve ser avaliado

4. SENSIBILIDADE AO DATASET:
   ✗ Features selecionadas podem não generalizar para novos participantes
   ✗ Distribuição diferente → features diferentes podem ser importantes
   ✗ Seleção pode ser enviesada por desbalanceamento de classes
   ✗ Outliers podem influenciar ranking (especialmente Fisher)

5. LIMITAÇÕES COMPUTACIONAIS DA SELEÇÃO:
   ✗ ReliefF pode ser custoso para datasets muito grandes
   ✗ Requer amostragem (100-1000 instâncias) para viabilidade
   ✗ Resultado pode variar com amostragem (não-determinístico)
   ✗ Necessário repetir seleção se dataset mudar

6. QUESTÕES PRÁTICAS:
   ✗ Pipeline de inferência deve replicar exatamente a seleção
   ✗ Manutenção: código deve rastrear quais features são usadas
   ✗ Versionamento: mudanças em seleção requerem retreinamento
   ✗ Debugging mais difícil (menos visibilidade de features descartadas)

7. LIMITAÇÕES ESPECÍFICAS POR MÉTODO:

   Fisher Score:
   ✗ Assume distribuições gaussianas (nem sempre válido)
   ✗ Análise univariada (ignora interações entre features)
   ✗ Sensível a outliers (média e variância)
   ✗ Pode falhar em separabilidade não-linear

   ReliefF:
   ✗ Custo O(m×n×d): alto para grandes datasets
   ✗ Sensível à escolha de k (número de vizinhos)
   ✗ Não-determinístico (amostragem aleatória)
   ✗ Pode ser instável com poucos dados

ANÁLISE DE TRADE-OFFS:

1. PERFORMANCE vs. SIMPLICIDADE:
   - 66 features: máxima informação, maior complexidade
   - 14 features (união): bom equilíbrio, 78% compressão
   - 10 features (consenso): máxima simplicidade, risco de perda

   Recomendação: Usar união (14 features) como ponto de partida

2. VELOCIDADE vs. PRECISÃO:
   - Menos features = inferência mais rápida
   - Mas: pode reduzir acurácia em 1-3%
   
   Recomendação: Validar empiricamente o trade-off

3. INTERPRETABILIDADE vs. PERFORMANCE:
   - 10 features: altamente interpretável
   - 66 features: melhor performance potencial
   
   Recomendação: Depende do caso de uso (produção vs. pesquisa)

ESTRATÉGIAS DE MITIGAÇÃO:

1. VALIDAÇÃO RIGOROSA:
   → Testar em holdout set independente
   → Comparar performance: 66 features vs. selecionadas
   → Se perda < 2%, seleção é válida

2. SELEÇÃO ENSEMBLE:
   → Combinar Fisher, ReliefF e outros métodos
   → Usar votação para features mais robustas
   → Considerar top-15 ou top-20 para mais segurança

3. ANÁLISE DE SENSIBILIDADE:
   → Testar com diferentes valores de k (top-10, top-15, top-20)
   → Avaliar estabilidade da seleção com re-amostragem
   → Verificar se features consensuais mantêm-se estáveis

4. MONITORAMENTO CONTÍNUO:
   → Em produção, monitorar performance com features selecionadas
   → Detectar drift: features selecionadas podem perder relevância
   → Re-executar seleção periodicamente se dataset evolui

RECOMENDAÇÕES FINAIS:

Para este projeto (HAR com 66 features):

1. EXPLORAÇÃO (fase atual):
   ✓ Usar AMBOS Fisher Score e ReliefF
   ✓ Analisar features comuns (alta confiança)
   ✓ Investigar features únicas de cada método
   ✓ Documentar: 14 features da união são candidatas

2. DESENVOLVIMENTO DE MODELO:
   ✓ Treinar modelos com: 66, 14, 10 features
   ✓ Comparar performance (acurácia, F1, tempo)
   ✓ Validar cross-validation com diferentes subconjuntos
   ✓ Escolher configuração com melhor trade-off

3. PRODUÇÃO:
   ✓ Se performance degrada < 2%: usar 14 features selecionadas
   ✓ Se aplicação crítica: manter 66 features
   ✓ Se dispositivo embedded: prioritizar 10 features mais leves
   ✓ Documentar pipeline de seleção para reprodutibilidade

CONCLUSÃO:

A seleção de features é uma técnica poderosa para reduzir dimensionalidade
e melhorar eficiência, mas deve ser aplicada com validação rigorosa. O uso
combinado de Fisher Score e ReliefF oferece perspectivas complementares
(global/local) e aumenta confiança nas features selecionadas. A decisão final
entre usar features selecionadas ou completas deve ser baseada em validação
empírica considerando o trade-off entre simplicidade e performance para o
caso de uso específico.

No contexto deste projeto, a redução de 66 para 14 features (78% compressão)
oferece ganhos substanciais em eficiência com risco controlado de perda de
performance, sendo recomendada como abordagem inicial para desenvolvimento
de modelos de classificação de atividades humanas.

================================================================================
REFERÊNCIAS
================================================================================

1. Teste Kolmogorov-Smirnov:
   https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html

2. Escolha de Testes Estatísticos:
   https://pmc.ncbi.nlm.nih.gov/articles/PMC2881615/

3. Feature Extraction (Zhang & Sawchuk):
   https://www.semanticscholar.org/paper/A-feature-selection-based-framework-for-human-using-Zhang-Sawchuk/8522ce2bfce1ab65b133e411350478183e79fae7

4. Fisher Score (Gu et al.):
   Q. Gu, Z. Li, J. Han (2012) "Generalized Fisher Score for Feature Selection"
   https://arxiv.org/abs/1202.3725

5. ReliefF (Kononenko):
   I. Kononenko (1994) "Estimating Attributes: Analysis and Extensions of RELIEF"
   Machine Learning, ECML-94

6. ReliefF Analysis (Robnik-Šikonja & Kononenko):
   M. Robnik-Šikonja, I. Kononenko (2003) "Theoretical and Empirical Analysis of ReliefF and RReliefF"
   Machine Learning Journal

7. Feature Selection (Guyon & Elisseeff):
   I. Guyon, A. Elisseeff (2003) "An Introduction to Variable and Feature Selection"
   Journal of Machine Learning Research

8. Curse of Dimensionality (Bellman):
   R. Bellman (1961) "Adaptive Control Processes: A Guided Tour"
   Princeton University Press

================================================================================
FIM DO DOCUMENTO
================================================================================
